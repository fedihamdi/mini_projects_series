{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hevCfAwasxtZ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run LLM Locally : A Step-by-Step Guide"
      ],
      "metadata": {
        "id": "uAqgmfhwrakx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction üåü"
      ],
      "metadata": {
        "id": "TI2OayiUreqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We‚Äôll leverage tools like GPT4All and llama-cpp-python to set up and run LLMs on your local laptop for enhanced privacy and cost-effectiveness."
      ],
      "metadata": {
        "id": "a7P8g7Dnrju6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Your Environment üõ†Ô∏è"
      ],
      "metadata": {
        "id": "16HYF7irrmlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS3QdNSkrNox"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade llama-cpp-python langchain gpt4all llama-index sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run LLM Locally üè°: 1st attempt\n",
        "\n",
        ">> Download the Model üöÄ"
      ],
      "metadata": {
        "id": "TwcRwDaLrsTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/Llama-2‚Äì7b-Chat-GGUF llama-2‚Äì7b-chat.Q5_K_S.gguf ‚Äî local-dir . ‚Äî local-dir-use-symlinks False"
      ],
      "metadata": {
        "id": "VhYkNAxTrp2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> Load and Use the Model üöÄ"
      ],
      "metadata": {
        "id": "e6Df02QIr2h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "oH4i2_nksU1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "llm = Llama(model_path='/content/llama-2-7b-chat.Q5_K_S.gguf')"
      ],
      "metadata": {
        "id": "S0qwo6NXr0lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can complete sentences using your LLM:"
      ],
      "metadata": {
        "id": "E3TtN5kjr5da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The capital of Tunisia is \"\n",
        "response = llm(sentence)\n",
        "print(response['choices'][0]['text'])"
      ],
      "metadata": {
        "id": "4AYtbm3Cr7mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manage Personal Data üóÉÔ∏è"
      ],
      "metadata": {
        "id": "Sm2T7WIjr9sY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "For simplicity, we‚Äôll use a text file as our data source. Later, you can copy and paste information from your calendar, news, articles, books, etc. Here‚Äôs an example of a mock calendar:\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JNOs2_tIsDd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./data.txt\n",
        "# data.txt\n",
        "\n",
        "11 Jan - Go to the movie\n",
        "12 Jan - Have a dinner with family\n",
        "13 Jan - Go to the birthday party\n",
        "14 Jan - Go to the dentist\n",
        "15 Jan - Finish reviewing a git pull request"
      ],
      "metadata": {
        "id": "jWU6rsjrr_rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Necessary Components üß©"
      ],
      "metadata": {
        "id": "FYs9-epzsLqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from llama_index import PromptHelper\n",
        "from llama_index.node_parser import SentenceSplitter"
      ],
      "metadata": {
        "id": "f6EKg3sRsOo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An embedding model used to structure text into representations\n",
        "embed_model = LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        ")\n",
        "\n",
        "# PromptHelper can help deal with LLM context window and token limitations\n",
        "prompt_helper = PromptHelper(context_window=2048)\n",
        "\n",
        "# SentenceSplitter used to split our data into multiple chunks\n",
        "# Only a number of relevant chunks will be retrieved and fed into LLMs\n",
        "node_parser = SentenceSplitter(chunk_size=300, chunk_overlap=20)"
      ],
      "metadata": {
        "id": "31Eq2P_lsR_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up LlamaCPP for Embedding and Sentence Handling"
      ],
      "metadata": {
        "id": "XWxU63IpsaRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for embedding, prompt helper, and sentence splitter\n",
        "from llama_index.llms import LlamaCPP\n",
        "from llama_index.llms.llama_utils import (\n",
        "    messages_to_prompt,\n",
        "    completion_to_prompt,\n",
        ")\n",
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGML model to download it automatically\n",
        "    model_url=None,#model_url,\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path='llama-2-7b-chat.Q5_K_S.gguf',\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=3900,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": -1},# -1 for selecting all cores\n",
        "    # transform inputs into Llama2 format\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "PyKJ9L3TsWiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context Service üì¶"
      ],
      "metadata": {
        "id": "8f41UmjusgWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import ServiceContext"
      ],
      "metadata": {
        "id": "fpMYs_LWshGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm,\n",
        "    embed_model=embed_model,\n",
        "    prompt_helper=prompt_helper,\n",
        "    node_parser=node_parser\n",
        ")"
      ],
      "metadata": {
        "id": "tT_ovMFLsizA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build an Index and Query Engine üìä"
      ],
      "metadata": {
        "id": "W91FX0pqsm_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import SimpleDirectoryReader\n",
        "from llama_index import VectorStoreIndex"
      ],
      "metadata": {
        "id": "00z_Uc5Yskmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data.txt into a document\n",
        "document = SimpleDirectoryReader(input_files=['./data.txt']).load_data()\n",
        "\n",
        "# Process data (chunking, embedding, indexing) and store them\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    document, service_context=service_context)\n",
        "\n",
        "# Build a query engine from the index\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "Y7pjkbmfspW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query('Give me my calendar.')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "QLgrSc57srei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What's the plan for 13 Jan?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "kPgNXvhVsuuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next steps: Summarize a book üìö"
      ],
      "metadata": {
        "id": "hevCfAwasxtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cILxWKI6s039"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion üéâ"
      ],
      "metadata": {
        "id": "_bk8ORuGs3ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You‚Äôve successfully created your personal assistant."
      ],
      "metadata": {
        "id": "fI78KQ8qs5uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sources :\n",
        "\n",
        "GitHub : https://github.com/fedihamdi\n",
        "\n",
        "LlamaIndex : https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp.html\n",
        "\n",
        "HuggingFace : https://huggingface.co/docs/huggingface_hub/guides/cli\n",
        "\n",
        "Gpt4all : https://gpt4all.io/index.html"
      ],
      "metadata": {
        "id": "_bqUV2Ies-GI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4_ZsWrCs_a3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}